2019-01-06 00:35:19  [ main:0 ] - [ INFO ]  ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [192.168.1.110:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-01-06 00:35:21  [ main:2170 ] - [ INFO ]  Kafka version : 2.1.0
2019-01-06 00:35:21  [ main:2187 ] - [ INFO ]  Kafka commitId : eec43959745f444f
2019-01-06 00:35:43  [ kafka-producer-network-thread | producer-1:23426 ] - [ WARN ]  [Producer clientId=producer-1] Connection to node -1 (/192.168.1.110:9092) could not be established. Broker may not be available.
2019-01-06 00:42:27  [ main:0 ] - [ INFO ]  ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [192.168.1.110:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-01-06 00:42:28  [ main:924 ] - [ INFO ]  Kafka version : 2.1.0
2019-01-06 00:42:28  [ main:930 ] - [ INFO ]  Kafka commitId : eec43959745f444f
2019-01-06 00:42:29  [ kafka-producer-network-thread | producer-1:1799 ] - [ WARN ]  [Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {stream-test1=LEADER_NOT_AVAILABLE}
2019-01-06 00:42:29  [ kafka-producer-network-thread | producer-1:1799 ] - [ INFO ]  Cluster ID: H6aITZJnSsWHz_zHVESg7g
2019-01-06 00:42:29  [ kafka-producer-network-thread | producer-1:2083 ] - [ WARN ]  [Producer clientId=producer-1] Error while fetching metadata with correlation id 3 : {stream-test1=LEADER_NOT_AVAILABLE}
2019-01-06 00:42:30  [ kafka-producer-network-thread | producer-1:2221 ] - [ WARN ]  [Producer clientId=producer-1] Error while fetching metadata with correlation id 4 : {stream-test1=LEADER_NOT_AVAILABLE}
2019-01-06 00:42:30  [ kafka-producer-network-thread | producer-1:2348 ] - [ WARN ]  [Producer clientId=producer-1] Error while fetching metadata with correlation id 5 : {stream-test1=LEADER_NOT_AVAILABLE}
2019-01-06 00:42:30  [ main:2623 ] - [ INFO ]  Topic = stream-test1-0@0, Offset = 0, Partition = 0
2019-01-06 00:42:32  [ main:4628 ] - [ INFO ]  Topic = stream-test1-0@1, Offset = 1, Partition = 0
2019-01-06 00:54:54  [ main:0 ] - [ INFO ]  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.1.110:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-01-06 00:54:55  [ main:1003 ] - [ INFO ]  Kafka version : 2.1.0
2019-01-06 00:54:55  [ main:1007 ] - [ INFO ]  Kafka commitId : eec43959745f444f
2019-01-06 00:54:55  [ kafka-producer-network-thread | producer-1:1721 ] - [ INFO ]  Cluster ID: H6aITZJnSsWHz_zHVESg7g
2019-01-06 00:54:55  [ main:1783 ] - [ INFO ]  Topic = stream-test1-0@2, Offset = 2, Partition = 0
2019-01-06 00:55:00  [ main:6788 ] - [ INFO ]  Topic = stream-test1-0@3, Offset = 3, Partition = 0
2019-01-06 00:56:08  [ main:0 ] - [ INFO ]  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.1.110:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-01-06 00:56:10  [ main:2008 ] - [ INFO ]  Kafka version : 2.1.0
2019-01-06 00:56:10  [ main:2014 ] - [ INFO ]  Kafka commitId : eec43959745f444f
2019-01-06 00:56:11  [ kafka-producer-network-thread | producer-1:2659 ] - [ INFO ]  Cluster ID: H6aITZJnSsWHz_zHVESg7g
2019-01-06 00:56:11  [ main:2719 ] - [ INFO ]  Topic = stream-test1-0@4, Offset = 4, Partition = 0
2019-01-06 00:56:16  [ main:7732 ] - [ INFO ]  Topic = stream-test1-0@5, Offset = 5, Partition = 0
2019-01-06 00:56:21  [ main:12736 ] - [ INFO ]  Topic = stream-test1-0@6, Offset = 6, Partition = 0
2019-01-06 00:56:25  [ main:0 ] - [ INFO ]  StreamsConfig values: 
	application.id = wordcount-application
	application.server = 
	bootstrap.servers = [192.168.1.110:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	max.task.idle.ms = 0
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	topology.optimization = none
	upgrade.from = null
	windowstore.changelog.additional.retention.ms = 86400000

2019-01-06 00:56:26  [ main:17741 ] - [ INFO ]  Topic = stream-test1-0@7, Offset = 7, Partition = 0
2019-01-06 00:56:26  [ main:776 ] - [ INFO ]  AdminClientConfig values: 
	bootstrap.servers = [192.168.1.110:9092]
	client.dns.lookup = default
	client.id = wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-admin
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2019-01-06 00:56:26  [ main:937 ] - [ INFO ]  Kafka version : 2.1.0
2019-01-06 00:56:26  [ main:938 ] - [ INFO ]  Kafka commitId : eec43959745f444f
2019-01-06 00:56:26  [ main:965 ] - [ INFO ]  stream-thread [wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1] Creating restore consumer client
2019-01-06 00:56:26  [ main:972 ] - [ INFO ]  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [192.168.1.110:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-01-06 00:56:27  [ main:1283 ] - [ INFO ]  Kafka version : 2.1.0
2019-01-06 00:56:27  [ main:1284 ] - [ INFO ]  Kafka commitId : eec43959745f444f
2019-01-06 00:56:27  [ main:1293 ] - [ INFO ]  stream-thread [wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1] Creating shared producer client
2019-01-06 00:56:27  [ main:1305 ] - [ INFO ]  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.1.110:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-01-06 00:56:27  [ main:1404 ] - [ INFO ]  Kafka version : 2.1.0
2019-01-06 00:56:27  [ main:1404 ] - [ INFO ]  Kafka commitId : eec43959745f444f
2019-01-06 00:56:27  [ main:1435 ] - [ INFO ]  stream-thread [wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1] Creating consumer client
2019-01-06 00:56:27  [ main:1482 ] - [ INFO ]  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.1.110:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = wordcount-application
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-01-06 00:56:27  [ main:1578 ] - [ WARN ]  The configuration 'admin.retries' was supplied but isn't a known config.
2019-01-06 00:56:27  [ main:1578 ] - [ INFO ]  Kafka version : 2.1.0
2019-01-06 00:56:27  [ main:1620 ] - [ INFO ]  Kafka commitId : eec43959745f444f
2019-01-06 00:56:27  [ main:1659 ] - [ INFO ]  stream-client [wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a] Started Streams client
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:1664 ] - [ INFO ]  stream-thread [wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1] Starting
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:1664 ] - [ INFO ]  stream-thread [wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1] State transition from CREATED to RUNNING
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:1720 ] - [ INFO ]  Cluster ID: H6aITZJnSsWHz_zHVESg7g
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:1722 ] - [ INFO ]  [Consumer clientId=wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1-consumer, groupId=wordcount-application] Discovered group coordinator 192.168.1.110:9092 (id: 2147483647 rack: null)
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:1743 ] - [ INFO ]  [Consumer clientId=wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1-consumer, groupId=wordcount-application] Revoking previously assigned partitions []
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:1744 ] - [ INFO ]  stream-thread [wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1] State transition from RUNNING to PARTITIONS_REVOKED
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:1744 ] - [ INFO ]  stream-client [wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a] State transition from RUNNING to REBALANCING
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:1746 ] - [ INFO ]  [Consumer clientId=wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1-restore-consumer, groupId=] Unsubscribed all topics or patterns and assigned partitions
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:1746 ] - [ INFO ]  stream-thread [wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1] partition revocation took 1 ms.
	suspended active tasks: []
	suspended standby tasks: []
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:1747 ] - [ INFO ]  [Consumer clientId=wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1-consumer, groupId=wordcount-application] (Re-)joining group
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:1924 ] - [ INFO ]  stream-thread [wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1-consumer] Assigned tasks to clients as {57dd4d1b-a314-47a5-aacc-98360deabc7a=[activeTasks: ([0_0]) standbyTasks: ([]) assignedTasks: ([0_0]) prevActiveTasks: ([]) prevStandbyTasks: ([]) prevAssignedTasks: ([]) capacity: 1]}.
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:2041 ] - [ INFO ]  [Consumer clientId=wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1-consumer, groupId=wordcount-application] Successfully joined group with generation 1
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:2046 ] - [ INFO ]  [Consumer clientId=wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1-consumer, groupId=wordcount-application] Setting newly assigned partitions [stream-test1-0]
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:2047 ] - [ INFO ]  stream-thread [wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1] State transition from PARTITIONS_REVOKED to PARTITIONS_ASSIGNED
2019-01-06 00:56:27  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:2103 ] - [ INFO ]  stream-thread [wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1] partition assignment took 56 ms.
	current active tasks: [0_0]
	current standby tasks: []
	previous active tasks: []

2019-01-06 00:56:28  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:2121 ] - [ INFO ]  [Consumer clientId=wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1-restore-consumer, groupId=] Unsubscribed all topics or patterns and assigned partitions
2019-01-06 00:56:28  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:2121 ] - [ INFO ]  [Consumer clientId=wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1-restore-consumer, groupId=] Unsubscribed all topics or patterns and assigned partitions
2019-01-06 00:56:28  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:2122 ] - [ INFO ]  stream-thread [wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING
2019-01-06 00:56:28  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:2123 ] - [ INFO ]  stream-client [wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a] State transition from REBALANCING to RUNNING
2019-01-06 00:56:28  [ wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1:2215 ] - [ INFO ]  [Consumer clientId=wordcount-application-57dd4d1b-a314-47a5-aacc-98360deabc7a-StreamThread-1-consumer, groupId=wordcount-application] Resetting offset for partition stream-test1-0 to offset 0.
2019-01-06 00:56:31  [ main:22756 ] - [ INFO ]  Topic = stream-test1-0@8, Offset = 8, Partition = 0
2019-01-06 00:56:36  [ main:27785 ] - [ INFO ]  Topic = stream-test1-0@9, Offset = 9, Partition = 0
